# CPP_PRALLEL_CORPUS
We introduce a new parallel corpus for the C-Plus-Plus language .

A parallel corpus comprising of a code snippet and the corresponding natural language description can be of great advantage. This can be used for creating several useful productivity tools like code generation, code completion, code summarization, CODE SEARCH etc. We see many such parallel corpus for the languages like Java, python, golang, JavaScript, BASH what we propose is to prepare one such corpus for c++ programming language. 
We would be specifically targeting the code summarization task i.e. given a code the model is required to generate a natural language description of it. 



## Data Preparation Cycle

![data_cycle](https://github.com/pritam004/CPP_corpus/blob/main/process.png?raw=true)


## Download links and Statistics

|PART-ID | MONO | GOOD PARA |BAD PARA|
|:---:|:---:|:---:|:---:|
|1|[487290](https://drive.google.com/file/d/1nL3RlGsbjCF8d5PK57o5XmwfdBUFucIG/view?usp=sharing)|[127977](https://drive.google.com/file/d/141ZHZiLkzoXjBjFd2iun6ukLfhey9xFK/view?usp=sharing)|[81704](https://drive.google.com/file/d/1M6xlaekc3N5RdQ4bJPGWmiSHqmOp8y0G/view?usp=sharing)|
|2|[561590](https://drive.google.com/file/d/1oPNX3UNSTeyTK610PL21ciS1450NShOq/view?usp=sharing)|[178075](https://drive.google.com/file/d/1Qt79_ismezyRXfGMu_DF8T8u5RwjdtaK/view?usp=sharing)|[111788](https://drive.google.com/file/d/1_ty2FTBdHOeXZTn2sF3BhjzkKJXY6DSf/view?usp=sharing)|
|3|[470246](https://drive.google.com/file/d/1rYwr0YMworAAaCQ8XgGkPJkF8355aPrU/view?usp=sharing)|[168927](https://drive.google.com/file/d/1KZ14nVbO-RqoqFwpI17YotB2k44NmYbC/view?usp=sharing)|[90165](https://drive.google.com/file/d/1HCCxlEKD9b_ZRz3hIvr3X09FbWsWz9p3/view?usp=sharing)|
|4|[484018](https://drive.google.com/file/d/1RsBu8HBQgTD8YitR5eP9MJmh3sCLJaPe/view?usp=sharing)|[145623](https://drive.google.com/file/d/1-4MerqavZVeC32gBeTpDn65TfJXVCfm2/view?usp=sharing)|[89253](https://drive.google.com/file/d/1WqZ4w3OD3Zi0Ulq2zNI9YVI7GB08MR6k/view?usp=sharing)
|5|[466051](https://drive.google.com/file/d/1rgFdADUvnz6uROMaxjDHesejguO0v30D/view?usp=sharing)|[190387](https://drive.google.com/file/d/1tvgGBeFHZb5gAYgnRwbhm7KU6Lf31op5/view?usp=sharing)|[89400](https://drive.google.com/file/d/1yGTkjf6yvd6XpZ76DALBF_OGYHEzCPuW/view?usp=sharing)|

*A couple of more parts are under process and it will be added soon


## Baseline

We also provide a lstm-attention-translation-model as a baseline. It is in the Baseline folder.

It achieves a BLUE of <b>24.21</b> on a deduplicated test set of <b>9,083</b> samples.We have made sure that the test set is unique and it is not a subset of the train set, but the train set may contain duplicates. Deduplication check was quite simple, it was pair-wise exact matching that was checked. 

The model was trained on a subset of the data available, like 3/5th of it. The extracted train and test set can be downloaded from [here](https://drive.google.com/drive/folders/1C9DEct6Qjk9zQzuBaT1sIKsee5BljdCb?usp=sharing).
The tokenization used was simple whitespace tokenizer which is used in many architectures.Since it is a simple baseline model it solves our purpose here but for more advanced models seperate processing needs to be performed. The vocabulary for the train_set can be found here. This is what we use for buiding the model . 

The outputs generated by this model can be found in the output folder of the Baseline folder. The trained model (which takes quite some some time) can be downloaded from [here](https://drive.google.com/drive/folders/1glM0BWVXGTJ141DsbsU6l_zynPe0uYLV?usp=sharing).

### Here are some example generation scripts

<code>count  int QWebHistory :: count ( ) const { return d -> lst -> entries ( ) . size ( ); }</code>

<b>predicted o/p:</b> Returns the number of items in the list.

<b>ground truth:</b> Returns the total number of items in the history.

<code> virtual bool SkWStream write  write  bool SkWStream :: write 16 ( U16CPU value ) { uint 16_t v = SkToU 16 ( value ); return this -> write ( & v , 2 ); } </code>

<b>predicted o/p:</b>Get the features that are supported by this device.

<b>ground truth:</b>Write a value to the stream.


The codes for the baseline model have been borrowed from the Stanford Course 224N on DLNLP, Assignment 3 and the code is my solution of the assignment but the utils files are those provided by the instructors.


## How to reproduce the dataset 

The steps are as follows and has to be carried out with exactness to reproduce the results. The data preparation life cycle can be referenced to understand it better. To make the processes efficient most of them have been parallelized and makes active use of multiple threads.

1. First step is to get the list of repositories that we want to use. For this task We have used   [libraries.io](libraries.io)   data dump and extract the relevant repositories with atleast 25 stars and some other heuristics. 
2. Install Doxygen, which is the core module in our experiments.

    2.1  <code>git clone https://github.com/doxygen/doxygen</code> 

    2.2 <code> git checkout 6922d5d63d77c8f640c58e9c68a9955f9f0aa9a7 </code> [This is important we need this specific commit]

    2.3 Build doxygen using the guide given in their website and set the environment paths.
3. Once we have the repositories and doxygen, run the runner.py 
   
    <code>python runner.py <number_of_repo></code>

    This calls first doxygen and the scrapper to dump the data in json. 
4. Next step is to generate the corpuses.

    4.1  <code>mkdir para mono residue</code>

    4.2  <code>python run_get_corpus.py</code>
5. Next step is to combine the corpus tp form a single file for mono and para respectively.

    5.1 <code>python combine_corpus.py para</code>

    5.2 <code>python combine_corpus.py mono</code>

## How does the dataset look

![data](https://github.com/pritam004/CPP_corpus/blob/main/data.png?raw=true)




